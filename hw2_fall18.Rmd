---
title: "Modern Data Mining - HW 2"
author:
- Anirudh Bajaj
- Esther Shin
- Matt LeBaron
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.height=5, fig.width=11, warning = F)

# constants for homework assignments
hw_num <- 2
hw_due_date <- "Oct 10, 2018"
```

## Overview / Instructions

This is homework #`r paste(hw_num)` of STAT 471/571/701. It will be **due on `r paste(hw_due_date)` by 11:59 PM** on Canvas. You can directly edit this file to add your answers. Submit the Rmd file, a PDF or word or HTML version with only 1 submission per HW team.

```{r library, include=FALSE}
# add your library imports here:
library(dplyr)
library(leaps) # regsubsets(): model selection
library(ggplot2)
library(ISLR)
```

## Problem 0

Review the code and concepts covered during lecture: multiple regression, model selection and penalized regression through elastic net. 

## Problem 1

Do ISLR, page 262, problem 8 only part (a) to (d) and write up the answer here. This question is designed to help us understanding model selections through simulations. 
(e) Describe as accurate as possible what Cp and BIC are estimating?

(a) 

```{r, eval = T}
# Use rnorm() to generate a predictor X of length n = 100, and a noise vector of length n = 100
x <- rnorm(100)
noise <- rnorm(100)
```

(b)

```{r, eval = T}
# Generate a response vector Y of length n = 100 according to the model
y <- 1 + 2*x + 3*x^2 + 4*x^3 + noise
```

(c)

```{r, eval = T}
# Create the predictors x^2 through x^10
x2 <- x^2
x3 <- x^3
x4 <- x^4
x5 <- x^5
x6 <- x^6
x7 <- x^7
x8 <- x^8
x9 <- x^9
x10 <- x^10

# Use the regsubsets() function to perform best subset selection
new_data <- data.frame(x,x2,x3,x4,x5,x6,x7,x8,x9,x10,noise,y)
new_subset <- regsubsets(y ~ x+x2+x3+x4+x5+x6+x7+x8+x9+x10,data = new_data, nvmax = 10)
new_summary <- summary(new_subset)
new_summary$which

data.frame(variables=(1:length(new_summary$rsq)), r_squared=new_summary$rsq)

# What is the best model obtained?
data.frame(variables = (1:length(new_summary$rsq)),
           r_squared = new_summary$rsq,
           rss = new_summary$rss,
           bic = new_summary$bic,
           cp = new_summary$cp)
```

The best model obtained has three variables: x, x2, and x3. This makes sense because these three variables were used to generate Y in the first place, so they should be most predictive, although they aren't perfect because we added in noise as well.

```{r, eval = T}
# Comparing the plots for Cp, bic, and r2
plot(new_summary$cp, xlab="Number of predictors", 
     ylab="cp", col="red", type="p", pch=16)
plot(new_summary$bic, xlab="Number of predictors", 
     ylab="bic", col="blue", type="p", pch=16)
plot(new_summary$adjr2, xlab="Number of predictors", 
     ylab="adjr2", col="green", type="p", pch=16)

# Find the optimal model size for Cp
opt.size <- which.min(new_summary$cp)
opt.size

# Find the optimal model size for bic
opt.size <- which.min(new_summary$bic)
opt.size

# Find the optimal model size for r2
opt.size <- which.min(new_summary$rsq)
opt.size

# Report the coefficients of the best model obtained
coef(new_subset, 3)
```

(d)

```{r, eval = T}
# Repeat using forward selection
forward <- regsubsets(y ~ x+x2+x3+x4+x5+x6+x7+x8+x9+x10,data = new_data, nvmax = 10, method = "forward")
forward_sum <- summary(forward)
forward_sum

plot(forward_sum$rsq, ylab="rsq", col="red", type="p", pch=16,
     xlab="Forward Selection")

# Find the optimal model size for forward selection
opt.size <- which.min(forward_sum$rsq)
opt.size

# Report the coefficients for forward selection
coef(forward, 3)

# Repeat using backward selection
backward <- regsubsets(y ~ x+x2+x3+x4+x5+x6+x7+x8+x9+x10,data = new_data, nvmax = 10, method = "backward")
backward_sum <- summary(backward)
backward_sum

plot(backward_sum$rsq, ylab="rsq", col="red", type="p", pch=16,
     xlab="Backward Selection")

# Find the optimal model size for backward selection
opt.size <- which.min(backward_sum$rsq)
opt.size

# Report the coefficients for backward selection
coef(backward, 3)

```

Using forward and backward stepwise selection, we only use one variable (x3) as opposed to three variables (x, x2, x3).

(e)

Describe what Cp and BIC are estimating.

Cp: an unbiased estimator of average prediction errors.

BIC: uses k to estimate the number of parameters. It heavily penalizes additional variables when they are introduced into the model. Therefore, BIC models tend to have fewer variables.


## Problem 2:

This will be the last part of the Auto data from ISLR. The original data contains 408 observations about cars. It has some similarity as the data CARS that we use in our lectures. To get the data, first install the package ISLR. The data Auto should be loaded automatically. We use this case to go through methods learnt so far. 

You can access the necessary data with the following code:

```{r, eval = F}
# check if you have ISLR package, if not, install it
if(!requireNamespace('ISLR')) install.packages('ISLR') 
auto_data <- ISLR::Auto
```


 Final modelling question: we want to explore the effects of each feature as best as possible. <br /> You may explore interactions, feature transformations, higher order terms, or other strategies within reason. The model(s) should be as parsimonious (simple) as possible unless the gain in accuracy is significant from your point of view. Use Mallow's Cp or BIC to select the model.
  * Describe the final model and its accuracy. Include diagnostic plots with particular focus on the model residuals.
  * Summarize the effects found.
  * Predict the mpg of a car that is: built in 1983, in US, red, 180 inches long, 8 cylinders, 350 displacement, 260 as horsepower and weighs 4000 pounds. Give a 95% CI.


```{r}
#Taking a look at the data
dim(auto)
str(auto)

#Exclude non-numeric values, and run pairwise scatter plot
auto %>%
  select_if(is.numeric) %>%
  pairs()

#From this we can see that MPG is highly correlated with Displacement, Horsepower and Weight
```

```{r}
#Use Regsubsets to find the model with the smallest RSS 
names(auto)
data2 <- auto[,-9]
fit.exh <- regsubsets(mpg~ .,data2,nvmax = 25, method = "exhaustive")
names(fit.exh)
#List the model with the smallest RSS among each size of the model
summary(fit.exh)
f.e<-summary(fit.exh)
f.e$which
```

```{r}
data.frame(variables=(1:length(f.e$rsq)), r_squared=f.e$rsq)
#R2 increases as we increase number of variables (But much less incrementally so, after adding 2 variables)
```

```{r}
data.frame(variables = (1:length(f.e$rsq)),
           r_squared = f.e$rsq,
           rss = f.e$rss,
           bic = f.e$bic,
           cp = f.e$cp)
```

```{r}
coef(fit.exh, 6)
```

```{r}
coef(fit.exh, 7)
```

plots of $Cp$ vs number of predictors and plots of $BIC$ vs number of the predictors
```{r}
plot(f.e$cp, xlab="Number of predictors", 
     ylab="cp", col="red", type="p", pch=16)
#CP smallest at 6 variables
plot(f.e$bic, xlab="Number of predictors", 
     ylab="bic", col="blue", type="p", pch=16)
#BIC smallest at 3 variables
```

```{r}
which.min(f.e$cp)
which.min(f.e$bic) #Choose BIC
```
We may use 3 variable model (BIC tends to give the model with least number of predictors)

```{r}
fit.exh.var <- f.e$which[3,]
colnames(f.e$which)[fit.exh.var] 
```
#[1] "(Intercept)" "weight"      "year"        "origin" 
So we now have the three variables

```{r}
fit.final <- lm(mpg ~ weight + year + origin, data2) 
summary(fit.final)
```
Final model with three variables

```{r}
par(mfrow=c(1,2), mar=c(2.5,3,1.5,1), mgp=c(1.5,0.5,0))
plot(fit.final,1)
plot(fit.final,2)

```

#Assessment of the model ("Describe the final model and its accuracy. Include diagnostic plots with particular focus on the model residuals. Summarize the effects found."):
#MPG can be reasonably predicted by using three variables (weight, year, origin). The model has a good level of accuracy with an Adjusted Rsquare of 0.816. Diagnostic plot is included in the above chunk. 

#Making Predictions ("Predict the mpg of a car that is: built in 1983, in US, red, 180 inches long, 8 cylinders, 350 displacement, 260 as horsepower and weighs 4000 pounds. Give a 95% CI.")
```{r}
predicted.mpg <- data2[1,]
predicted.mpg$mpg <- NA
predicted.mpg$year <- 83
predicted.mpg$origin <- 1
predicted.mpg$cylinders <- 8
predicted.mpg$displacement <- 350
predicted.mpg$horsepower <- 260
predicted.mpg$weight <- 4000

predicted.mpg.final.model <- predict(fit.final, predicted.mpg, interval="confidence", se.fit=TRUE) 
print(predicted.mpg.final.model)
```
#fit      lwr      upr
#1 21.96954 21.01736 22.92172
#$se.fit
#[1] 0.4842996

#The predicted MPG for such car is 21.97 (21.02, 22.92)




## Problem 3: Lasso

Crime data continuation:  We use a subset of the crime data discussed in class, but only look at Florida and California. `crimedata` is available on Canvas; we show the code to clean here. 

```{r}
crime <- read.csv("CrimeData.csv", stringsAsFactors = F, na.strings = c("?"))
crime <- dplyr::filter(crime, state %in% c("FL", "CA"))
```

Our goal is to find the factors which relate to violent crime. This variable is included in crime as `crime$violentcrimes.perpop`.


Use LASSO to choose a reasonable, small model. Fit an OLS model with the variables obtained. The final model should only include variables with p-values < 0.05. Note: you may choose to use lambda 1st or lambda min to answer the following questions where apply. 

1. What is the model reported by LASSO? 

2. What is the model after running OLS?

3. What is your final model, after excluding high p-value variables?

Now, instead of Lasso, we want to consider how changing the value of alpha (i.e. mixing between Lasso and Ridge) will affect the model. Cross-validate between alpha and lambda, instead of just lambda. Note that the final model may have variables with p-values higher than 0.05; this is because we are optimizing for accuracy rather than parsimoniousness. 

1. What is your final elastic net model? What were the alpha and lambda values? What is the prediction error?

2. Use the elastic net variables in an OLS model. What is the equation, and what is the prediction error.

3. Summarize your findings, with particular focus on the difference between the two equations.
 

